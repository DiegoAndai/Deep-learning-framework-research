emb_dim: 500
num_samples: 25
window_size: 8
subsample: 0.0001
concurrent_steps: 12
epochs_to_train: 50
save_path: TrainedModels/Model4
train_data: ../../TrainingSets/allwiki
vocab_counts: [2887230 5471681 3133056 ...,       5       5       5]
vocab_size: 387635
min_count: 5
batch_size: 500
learning_rate: 0.05
eval_data: AnalogyTestingFiles/questions-words.txt
vocab_words: [b'UNK' b'the' b'of' ..., b'Negus' b'ly' b'Ossiriand,']
words_per_epoch: 83436520
